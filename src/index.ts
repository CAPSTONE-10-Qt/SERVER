import express, { NextFunction, Request, Response } from 'express';
import Configuration, { OpenAI } from 'openai';
import OpenAIApi from 'openai';
import config from './config';
import router from './router'
import * as dotenv from 'dotenv'
import errorHandler from './middleware/error/errorHandler';
import cors from 'cors';

dotenv.config()
const axios = require('axios')
const app = express(); // express ê°ì²´ ë°›ì•„ì˜´ 

const allowedOrigins = [
  'http://localhost:8000',
  'http://localhost:3000',
  config.ec2URL,
];
const corsOptions = {
  origin: allowedOrigins,
  credentials: true,
};

app.use(cors(corsOptions));
app.use((req, res, next) => {
  const origin: string = req.headers.origin!;
  if (allowedOrigins.includes(origin)) {
    res.setHeader('Access-Control-Allow-Origin', origin);
  }
  res.header('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, PATCH');
  res.header(
    'Access-Control-Allow-Headers',
    'X-Requested-With, content-type, x-access-token',
  );
  next();
});

app.use(express.json());
app.use(express.urlencoded({ extended: true }));
app.use('/', router);

const openai = new OpenAI({
  organization: "org-5IqT7dxuJeAfuyJEJJhg8VXq",
  apiKey: process.env.OPEN_API_KEY
});

export const Answer = async (questionText: string, text: string) => {
  try {
    const completion = await openai.completions.create({
      model: "gpt-3.5-turbo-instruct",
      prompt: "ë„ˆëŠ” ì»´í“¨í„°ê³µí•™ê³¼ êµìˆ˜ì•¼. ë’¤ì— ì˜¤ëŠ” ì»´ê³µ ì§ˆë¬¸ê³¼ í•™ìƒì˜ ëŒ€ë‹µì„ ë“£ê³  í‰ê°€í•´. êµ¬ì²´ì ìœ¼ë¡œ ì–´ëŠ ë¶€ë¶„ì€ ë§žì•˜ì§€ë§Œ ì–´ë–¤ ë¶€ë¶„ì€ ë¶€ì¡±í•œì§€ \"ì²¨ì‚­\"ì„ ìžì„¸ížˆ. í•™ìƒ ë‹µë³€ì„ ë‹¤ì‹œ ë³´ì—¬ì¤„ í•„ìš”ëŠ” ì—†ê³  300ìž ì•ˆìª½ìœ¼ë¡œ ê¼­ ë§ˆë¬´ë¦¬ë˜ê²Œ ë§í•´ì¤˜."+questionText+"ì´ê²Œ ì§ˆë¬¸ì´ê³  ë‹¤ìŒì´ í•™ìƒì˜ ë‹µë³€ì´ì•¼."+text,
      max_tokens: 400
    });
    const result = completion.choices[0].text;
    return result;
  } catch(error) {
    throw error;
  }
};

export const Score = async (questionText: string, text: string) => {
  try {
    const completion = await openai.completions.create({
      model: "gpt-3.5-turbo-instruct",
      prompt: "ë„ˆëŠ” ì»´í“¨í„°ê³µí•™ê³¼ êµìˆ˜ì•¼. ì§ˆë¬¸ê³¼ í•™ìƒì˜ ëŒ€ë‹µì„ 1ì , 0.5ì , 0ì  ì¤‘ì— í•˜ë‚˜ë¥¼ int ê°’ë§Œ ë¦¬í„´."+questionText+"ì´ê²Œ ì§ˆë¬¸ì´ê³  ë‹¤ìŒì´ í•™ìƒì˜ ë‹µë³€ì´ì•¼."+text,
      max_tokens: 10
    });
    const result = completion.choices[0].text;
    return result;
  } catch(error) {
    throw error;
  }
};

app.post('/api/chat', async (req, res) => {
  try {
  const { message } = req.body
  const completion = await openai.completions.create({
      model: "gpt-3.5-turbo-instruct",
      prompt: message,
      max_tokens:20
    });
  res.json({ response: completion.choices[0].text })
} catch (error) {
res.json({error})       
}
})

//Error Handler
interface ErrorType {
  message: string;
  status: number;
}

app.use(function (
  err: ErrorType,
  req: Request,
  res: Response,
  next: NextFunction,
) {
  res.locals.message = err.message;
  res.locals.error = req.app.get('env') === 'production' ? err : {};

  // render the error page
  res.status(err.status || 500);
  res.json({ error: err })
});

app.listen(config.port, () => {
  console.log(`
        #############################################
            ðŸ›¡ï¸ Server listening on port: ${config.port} ðŸ›¡ï¸
        #############################################
    `);
})
.on('error', (err) => {
  console.error(err);
  process.exit(1);
});

export default app;